\documentclass[12pt]{article}

\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian, english]{babel}
\usepackage{graphicx}
\usepackage{amsthm,amsmath,amssymb}
\usepackage[russian, english]{hyperref}
\usepackage{enumerate}
\usepackage{datetime}

\voffset=-20mm
\textheight=235mm
\hoffset=-25mm
\textwidth=180mm
\headsep=12pt
\footskip=20pt

\newenvironment{MyList}[1][4pt]{
	\begin{enumerate}[1.]
		\setlength{\parskip}{0pt}
		\setlength{\itemsep}{#1}
	}{       
	\end{enumerate}
}
\newenvironment{InnerMyList}[1][0pt]{
	\vspace*{-0.5em}
	\begin{enumerate}[a)]
		\setlength{\parskip}{#1}
		\setlength{\itemsep}{0pt}
	}{
	\end{enumerate}
}

\begin{document}	
	\textbf{Слайд 1}\\
	Здравствуйте! Мой проект: <<Improving Textual-Visual Cross-Modal Retrieval with Visual Attention to Text>>.
	
	\textbf{Слайд 2}\\
	Я расскажу о задаче, о существующих решениях и о том, как я решал данную задачу.
	
	\textbf{Слайд 3}\\
	Итак. Задача Cross-Modal Retrieval ставится следующим образом. Есть множество объектов типа \textbf{A} и дан запрос в виде объекта типа \textbf{B}. Нужно найти наиболее подходящие объекты типа \textbf{A}. В данном случае хочется иметь систему, которая по тексту сможет найти наиболее подходящие по описанию картинки и, соответственно, наоборот.
	
	Эта задача нужна для поиска. К примеру, в интернете полно визуального контента понятного человеку, но не имеющего текстового описания для обработки.
	
	\textbf{Слайд 4}\\
	Существующие решения можно разделить на три группы. Модели, которые принимают текст и картинку на вход, что-то там делают с ними и возвращают семантическую близость. Модели, строящие эмбеддинги в общем скрытом пространстве, где близость объектов пропорциональна некоторой функции расстояния. Такие решения подразумевают использования некоторого алгоритма хеширования эмбеддингов для эффективной работы. Снизу приведен общий вид таких моделей. И третий тип --- модели, которые непосредственно оптимизируют хеши. Они развивают идеи, вышедшие из алгоритма LSH.
	
	Модели первого типа в среднем наиболее точные, но они не эффективны, так как имеют линейное временя работы от количества данных. Последний же тип наоборот --- крайне эффективен по времени, но имеет худшую точность. В итог, модели второго типа наиболее популярны.
	
	Однако к всем этим решениям есть следующие претензии. Во-первых, они используют далеко не самые сильные текстовые модели. Во-вторых, они смотрят на текст и на изображение, как на равновесные объекты, хотя изображения --- это проекции реального мира, а вот текста в природе не существуют. Это абстракция, придуманная людьми, для \textbf{описания}. То есть, текст заранее содержит меньше информации. И, в-третьих, эти модели либо выдают ответ, никак его не проверяя, либо за линейное время проверяют каждый. Это, очевидно, две крайности. Со всеми вышеперечисленными проблемами нужно побороться.
	
	\textbf{Слайд 5}\\
	Но для начала про данные. Я использовал датасет MSCOCO. Это один из стандартных датасетов для данной задачи. Он содержит в общей сложности более $120$ тысяч изображений и по $5$ текстовых описаний к каждому.
	
	Оценивается модель следующим образом. Берем текст и $1000$ картинок. Сортируем картинки по расстоянию от текста в скрытом пространстве, и смотрим на положение парной к тексту картинки. Аналогично поступаем для всех картинок.
	
	\textbf{Слайд 6}\\
	Перейдем к архитектуре. За основу была взята одна из state-of-the-art моделей. Я заменил у нее текстовую модель на Transformer, который на данный момент является одной из, если не самой, сильной текстовой моделью и достигает state-of-the-art результатов на многих задачах NLP. А также добавил среднюю башня, использующую механизм visual-attention.
	
	Что происходит с данными? Сперва BPE разбивает текст на токены и строит для них эмбеддинги. По ним Transformer получает эмбеддинг текста, к которому применяется аффинное преобразование и L2 нормализация. С другой стороны фактически обычный ResNet, но использующий иной пуллинг. Затем также аффинное преобразование и L2 нормализация. Посередине же находится средняя башня, которая и реализует attention. О ней поговорим чуть подробнее.
	
	\textbf{Слайд 7}\\
	В чем суть attention'а? Если пуллинг берет feature-map размера $w \times h \times D$ и делает из него вектор размера $D$, не особо анализируя содержимое, то attention получает heat-map размера $w \times h$ и использует его, как коэффициенты для суммирования элементов feature-map'а.
	
	И основная идея здесь в способе получения heat-map'а, этот способ вытекает из логики самой модели. Смотрите, вот мы получили feature-map $G$ из ResNet'а. И мы хотим узнать, насколько каждый из его $w \times h$ элементов важен. Давайте сделаем с каждым из них то же самое, что раньше происходило с результатом пуллинга. То есть применим аффинное преобразование, что эквивалентно свертке $1 \times 1$. Получим новый feature-map $G'$. И с каждым из его $w \times h$ векторов посчитаем ту же функцию расстояния, что и раньше, то есть косинус. Значит нужно просто перемножить $G'$ и вектор-эмбеддинг текста $v$. Все, мы получили heat-map.
	
	\textbf{Слайд 8}\\
	Дальше. Для обучения модели используется Triplet Loss и Hard Negative Triplet Loss. Суть в том, что мы хотим добится ситуации, при которой парные объекты ближе друг к другу хотя бы на $\alpha$, чем любые непарные объекты. В Triplet Loss считается ошибка по всем сочетаниям непарных объектов, а в Hard Negative Triplet Loss учитываются только максимально отклоняющиеся сочетания.
	
	\textbf{Слайд 9}\\
	Давайте посмотрим на результаты. Модель с SRU --- это то, что было взято за основу, а с Transformer'ом --- моя модель. Здесь показаны варианты с обучением средней башни и без него. В каждых двух строчках точность для обычной башни и для средней. Caption retrieval --- поиск текста по картинке, image retrieval --- поиск картинки по тексту. R@k --- процент попадания парного объекта в k ближайших.
	
	Здесь можно видеть, что обучение средней башни дает сильный прирост точности. Причем точность увеличивается и на обычной башне, так как модель глубже, на более ранних этапах учится понимать, какие элементы изображения важны. Еще больший прирост получается при замене текстовой модели на Transformer. А лучшие результаты получаются при объединение обоих подходов. Причем image retrieval имеет более высокую точность на средней башни, а caption retrieval на обычной.
	
	Однако важно отметить, что в силу ограниченности вычислительных ресурсов глобальные параметры модели, такие как batch size и learning rate были изменены. И приведенные результаты актуальны именно для урезанных глобальных параметров, а не для оптимальных.
	
	\textbf{Слайд 10}\\
	Итого, что получилось? Во-первых, модель сильно выигрывает у state-of-the-art аналога на урезанных глобальных параметрах. Во-вторых, мы теперь можем в зависимости от задачи выбирать между быстродействием и точностью.
	
	Выводы. Еще раз показано, что Transformer сильнее SRU, а также, что усиление текстовой модели в задаче Cross-Modal Retrieval усиливает всю модель. Ну и то, что полезно обучать связь объектов на более ранних этапах.
	
	Вот. Есть ли какие-нибудь вопросы?
	
\end{document}